{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QebFSNmHZgQ"
   },
   "source": [
    "env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1748593021287,
     "user": {
      "displayName": "袁再权",
      "userId": "15431997863215513287"
     },
     "user_tz": -480
    },
    "id": "EtSxJwDTHeg9",
    "outputId": "9901d0ed-9bed-4ad8-a1fa-5e1ed4d17d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "玩家 -1 的有效行动: [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "玩家 1 的有效行动: [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# env.py\n",
    "from collections import deque  # 用于存储历史记录\n",
    "import random  # 用于随机打乱棋子\n",
    "from enum import Enum  # 用于定义枚举类型\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# 定义棋子类型的枚举\n",
    "class PieceType(Enum):\n",
    "    A = 1  # 类型 A\n",
    "    B = 2  # 类型 B\n",
    "    C = 3  # 类型 C\n",
    "    D = 4  # 类型 D\n",
    "\n",
    "# 定义棋子类\n",
    "class Piece:\n",
    "    def __init__(self, piece_type, player):\n",
    "        \"\"\"\n",
    "        初始化一个棋子。\n",
    "        Args:\n",
    "            piece_type (PieceType): 棋子的类型。\n",
    "            player (int): 拥有该棋子的玩家 (-1 或 1)。\n",
    "        \"\"\"\n",
    "        self.piece_type = piece_type  # 棋子类型\n",
    "        self.player = player  # 所属玩家\n",
    "        self.revealed = False  # 棋子是否已翻开，默认为 False\n",
    "\n",
    "# 定义游戏环境类\n",
    "class GameEnvironment:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化游戏环境。\n",
    "        \"\"\"\n",
    "        self.board = [[None for _ in range(4)] for _ in range(2)]  # 2x4 的棋盘，初始为空\n",
    "        self.dead_pieces = {-1:[],1:[]}\n",
    "        self.current_player = 1  # 当前回合的玩家，-1 或 1\n",
    "        self.move_counter = 0\n",
    "        self.max_move_counter = 16\n",
    "        self.scores = {-1:0,1:0}\n",
    "        self.init_board()  # 初始化棋盘布局\n",
    "\n",
    "    def init_board(self):\n",
    "        \"\"\"\n",
    "        初始化棋盘，随机放置双方的棋子。\n",
    "        \"\"\"\n",
    "        # 创建双方的棋子\n",
    "        pieces_player_neg1 = [Piece(PieceType.A, -1), Piece(PieceType.B, -1), Piece(PieceType.C, -1), Piece(PieceType.D, -1)]\n",
    "        pieces_player_1 = [Piece(PieceType.A, 1), Piece(PieceType.B, 1), Piece(PieceType.C, 1), Piece(PieceType.D, 1)]\n",
    "        pieces = pieces_player_neg1 + pieces_player_1  # 合并所有棋子\n",
    "        random.shuffle(pieces)  # 随机打乱棋子顺序\n",
    "\n",
    "        # 将棋子放置到棋盘上\n",
    "        idx = 0\n",
    "        for row in range(2):\n",
    "            for col in range(4):\n",
    "                self.board[row][col] = pieces[idx]  # 放置棋子\n",
    "                idx += 1\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置游戏环境到初始状态。\n",
    "        Returns:\n",
    "            dict: 游戏初始状态。\n",
    "        \"\"\"\n",
    "        self.__init__()  # 重新调用构造函数进行初始化\n",
    "        return self.get_state()  # 返回初始状态\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        获取当前游戏状态，返回一个包含 83 个特征值的 NumPy 数组。\n",
    "        \"\"\"\n",
    "        # 确定当前玩家和对手玩家\n",
    "        current_player = self.current_player\n",
    "        opponent_player = - current_player\n",
    "\n",
    "        # 1. 棋盘状态 (64个值)\n",
    "        my_pieces_planes = np.zeros((4, 2, 4), dtype=int)  # 己方棋子\n",
    "        opponent_pieces_planes = np.zeros((4, 2, 4), dtype=int)  # 对方棋子\n",
    "\n",
    "        for row in range(2):\n",
    "            for col in range(4):\n",
    "                piece = self.board[row][col]\n",
    "                if piece and piece.revealed:\n",
    "                    piece_type_index = piece.piece_type.value - 1  # 棋子类型索引 (0-3)\n",
    "                    if piece.player == current_player:\n",
    "                        my_pieces_planes[piece_type_index, row, col] = 1\n",
    "                    else:\n",
    "                        opponent_pieces_planes[piece_type_index, row, col] = 1\n",
    "\n",
    "        my_pieces_flattened = my_pieces_planes.flatten()  # 展平\n",
    "        opponent_pieces_flattened = opponent_pieces_planes.flatten()  # 展平\n",
    "\n",
    "        # 2. 死亡棋子状态 (8个值)\n",
    "        my_dead_pieces = np.zeros(4, dtype=int)\n",
    "        opponent_dead_pieces = np.zeros(4, dtype=int)\n",
    "\n",
    "        for piece in self.dead_pieces[current_player]:\n",
    "            piece_type_index = piece.piece_type.value - 1\n",
    "            my_dead_pieces[piece_type_index] = 1\n",
    "\n",
    "        for piece in self.dead_pieces[opponent_player]:\n",
    "            piece_type_index = piece.piece_type.value - 1\n",
    "            opponent_dead_pieces[piece_type_index] = 1\n",
    "\n",
    "        # 3. 隐藏棋子状态 (8个值)\n",
    "        hidden_pieces = np.zeros((2, 4), dtype=int)\n",
    "        for row in range(2):\n",
    "            for col in range(4):\n",
    "                piece = self.board[row][col]\n",
    "                if piece and not piece.revealed:\n",
    "                    hidden_pieces[row, col] = 1\n",
    "        hidden_pieces_flattened = hidden_pieces.flatten()\n",
    "\n",
    "        # 4. 得分状态 (2个值)\n",
    "        my_score = self.scores[current_player]\n",
    "        opponent_score = self.scores[opponent_player]\n",
    "\n",
    "        # 5. 计数器状态 (1个值)\n",
    "\n",
    "        # 组合所有特征\n",
    "        state = np.concatenate([\n",
    "            my_pieces_flattened,\n",
    "            opponent_pieces_flattened,\n",
    "            hidden_pieces_flattened,\n",
    "            my_dead_pieces,\n",
    "            opponent_dead_pieces,\n",
    "            [my_score, opponent_score],\n",
    "            [self.move_counter]\n",
    "        ])\n",
    "\n",
    "        return state  # 返回 NumPy 数组\n",
    "\n",
    "    def clone(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def step(self, action_index):\n",
    "        \"\"\"\n",
    "        执行一个动作。\n",
    "        Args:\n",
    "            action_index (int): 动作索引 (0-39)。\n",
    "        Returns:\n",
    "            tuple: 包含新的游戏状态、胜者和游戏是否结束的元组。\n",
    "        \"\"\"\n",
    "        # 解码动作索引\n",
    "        pos_idx = action_index // 5\n",
    "        action_sub_idx = action_index % 5\n",
    "        row = pos_idx // 4\n",
    "        col = pos_idx % 4\n",
    "        from_pos = (row, col)\n",
    "\n",
    "        # 根据动作类型执行相应操作\n",
    "        if action_sub_idx == 4:  # 翻开\n",
    "            self.reveal(from_pos)\n",
    "            self.move_counter = 0  # 翻棋重置移动计数\n",
    "        else:  # 移动或攻击\n",
    "            # 计算目标位置\n",
    "            d_row, d_col = 0, 0\n",
    "            if action_sub_idx == 0:  # 上\n",
    "                d_row = -1\n",
    "            elif action_sub_idx == 1:  # 下\n",
    "                d_row = 1\n",
    "            elif action_sub_idx == 2:  # 左\n",
    "                d_col = -1\n",
    "            elif action_sub_idx == 3:  # 右\n",
    "                d_col = 1\n",
    "            to_pos = (row + d_row, col + d_col)\n",
    "\n",
    "            target = self.board[to_pos[0]][to_pos[1]]\n",
    "            if target is None:  # 移动\n",
    "                self.move(from_pos, to_pos)\n",
    "                self.move_counter += 1  # 移动增加计数\n",
    "            else:  # 攻击\n",
    "                self.attack(from_pos, to_pos)\n",
    "                self.move_counter = 0  # 攻击重置移动计数\n",
    "\n",
    "        # 检查游戏是否结束\n",
    "        done = False\n",
    "        winner = None\n",
    "        if self.scores[1] >= 45:\n",
    "            winner = 1\n",
    "            done = True\n",
    "        elif self.scores[-1] >= 45:\n",
    "            winner = -1\n",
    "            done = True\n",
    "\n",
    "        if self.move_counter >= self.max_move_counter:\n",
    "            done = True\n",
    "            winner = 0\n",
    "\n",
    "        current_player = self.current_player #保存当前玩家以返回\n",
    "        # 切换玩家\n",
    "        self.current_player = -self.current_player\n",
    "        # 检查对手是否有有效行动\n",
    "        valid_actions = self.valid_actions()\n",
    "        if np.sum(valid_actions) == 0:\n",
    "            winner = -self.current_player\n",
    "            done = True\n",
    "\n",
    "        return self.get_state(), current_player, winner, done #下一个状态，当前玩家, 胜利者，是否结束\n",
    "    def reveal(self, position):\n",
    "        \"\"\"\n",
    "        翻开指定位置的棋子。\n",
    "        Args:\n",
    "            position (tuple): 要翻开棋子的位置 (row, col)。\n",
    "        \"\"\"\n",
    "        row, col = position\n",
    "        if self.board[row][col] is not None:\n",
    "            self.board[row][col].revealed = True\n",
    "\n",
    "    def move(self, from_pos, to_pos):\n",
    "        \"\"\"\n",
    "        将棋子从一个位置移动到另一个空位置。\n",
    "        Args:\n",
    "            from_pos (tuple): 起始位置 (row, col)。\n",
    "            to_pos (tuple): 目标位置 (row, col)。\n",
    "        \"\"\"\n",
    "        from_row, from_col = from_pos\n",
    "        to_row, to_col = to_pos\n",
    "        # 将棋子移动到目标位置，并将原位置置空\n",
    "        self.board[to_row][to_col] = self.board[from_row][from_col]\n",
    "        self.board[from_row][from_col] = None\n",
    "\n",
    "    def attack(self, from_pos, to_pos):\n",
    "        \"\"\"\n",
    "        用一个棋子攻击另一个位置的棋子。\n",
    "        Args:\n",
    "            from_pos (tuple): 攻击方棋子的位置 (row, col)。\n",
    "            to_pos (tuple): 防守方棋子的位置 (row, col)。\n",
    "        \"\"\"\n",
    "        from_row, from_col = from_pos\n",
    "        to_row, to_col = to_pos\n",
    "        attacker = self.board[from_row][from_col]\n",
    "        defender = self.board[to_row][to_col]\n",
    "\n",
    "        # 将被吃掉的棋子加入死亡列表\n",
    "        self.dead_pieces[defender.player].append(defender)\n",
    "        # 吃掉的棋子的敌对方得分增加\n",
    "        opponent = - defender.player\n",
    "        self.scores[opponent] += self.get_piece_value(defender.piece_type)\n",
    "        # 攻击方棋子移动到目标位置，并将原位置置空\n",
    "        self.board[to_row][to_col] = attacker\n",
    "        self.board[from_row][from_col] = None\n",
    "\n",
    "    def can_attack(self, attacker, defender):\n",
    "        \"\"\"\n",
    "        判断攻击方是否能吃掉防守方。\n",
    "        规则：\n",
    "        - D 不能吃 A。\n",
    "        - A 可以吃 D。\n",
    "        - 其他情况，类型值大的可以吃类型值小的或相等的。\n",
    "        Args:\n",
    "            attacker (Piece): 攻击方棋子。\n",
    "            defender (Piece): 防守方棋子。\n",
    "        Returns:\n",
    "            bool: 如果可以攻击则返回 True，否则返回 False。\n",
    "        \"\"\"\n",
    "        # 特殊规则：D 不能吃 A\n",
    "        if attacker.piece_type == PieceType.D and defender.piece_type == PieceType.A:\n",
    "            return False\n",
    "        # 特殊规则：A 可以吃 D\n",
    "        if attacker.piece_type == PieceType.A and defender.piece_type == PieceType.D:\n",
    "            return True\n",
    "        # 一般规则：类型值小的不能吃类型值大的\n",
    "        if attacker.piece_type.value < defender.piece_type.value:\n",
    "            return False\n",
    "        # 其他情况（类型值相等或更大）可以吃\n",
    "        return True\n",
    "\n",
    "    def get_piece_value(self, piece_type):\n",
    "        \"\"\"\n",
    "        获取指定棋子类型的分值。\n",
    "        Args:\n",
    "            piece_type (PieceType): 棋子类型。\n",
    "        Returns:\n",
    "            int: 棋子的分值。\n",
    "        \"\"\"\n",
    "        piece_values = {\n",
    "            PieceType.A: 10,\n",
    "            PieceType.B: 15,\n",
    "            PieceType.C: 15,\n",
    "            PieceType.D: 20,\n",
    "        }\n",
    "        return piece_values[piece_type]\n",
    "\n",
    "    def valid_actions(self):\n",
    "        \"\"\"\n",
    "        获取当前玩家所有合法的动作。\n",
    "        Returns:\n",
    "            numpy.ndarray: 大小为 40 的 NumPy 数组，表示每个动作是否合法。\n",
    "        \"\"\"\n",
    "        valid_actions_array = np.zeros(40, dtype=int)  # 初始化动作数组\n",
    "\n",
    "        # 遍历棋盘寻找当前玩家的棋子\n",
    "        for row in range(2):\n",
    "            for col in range(4):\n",
    "                piece = self.board[row][col]\n",
    "                if piece:\n",
    "                    # 如果棋子未翻开，可以执行 'reveal' 动作\n",
    "                    if not piece.revealed:\n",
    "                        action_index = (row * 4 + col) * 5 + 4  # 翻开动作索引\n",
    "                        valid_actions_array[action_index] = 1\n",
    "                    # 如果是当前玩家的已翻开棋子\n",
    "                    elif piece.player == self.current_player:\n",
    "                        # 检查相邻的四个方向\n",
    "                        for d_row, d_col in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                            new_row, new_col = row + d_row, col + d_col\n",
    "                            # 检查新位置是否在棋盘内\n",
    "                            if 0 <= new_row < 2 and 0 <= new_col < 4:\n",
    "                                action_sub_index = None\n",
    "                                if d_row == -1:\n",
    "                                    action_sub_index = 0  # 上\n",
    "                                elif d_row == 1:\n",
    "                                    action_sub_index = 1  # 下\n",
    "                                elif d_col == -1:\n",
    "                                    action_sub_index = 2  # 左\n",
    "                                elif d_col == 1:\n",
    "                                    action_sub_index = 3  # 右\n",
    "\n",
    "                                action_index = (row * 4 + col) * 5 + action_sub_index\n",
    "                                target = self.board[new_row][new_col]\n",
    "                                # 如果目标位置为空，可以执行 'move' 动作\n",
    "                                if target is None:\n",
    "                                    valid_actions_array[action_index] = 1\n",
    "                                # 如果目标位置是对方已翻开的棋子，并且可以攻击\n",
    "                                elif target.player != self.current_player and target.revealed and self.can_attack(piece, target):\n",
    "                                    valid_actions_array[action_index] = 1\n",
    "\n",
    "        return valid_actions_array\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = GameEnvironment()\n",
    "    # 翻开所有棋子\n",
    "    for row in range(2):\n",
    "        for col in range(4):\n",
    "            if env.board[row][col] is not None:\n",
    "                env.board[row][col].revealed = True\n",
    "\n",
    "    # 打印玩家 -1 的有效行动\n",
    "    env.current_player = -1\n",
    "    valid_actions_player_neg1 = env.valid_actions()\n",
    "    print(\"玩家 -1 的有效行动:\", valid_actions_player_neg1)\n",
    "\n",
    "    # 打印玩家 1 的有效行动\n",
    "    env.current_player = 1\n",
    "    valid_actions_player_1 = env.valid_actions()\n",
    "    print(\"玩家 1 的有效行动:\", valid_actions_player_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnatQI2SNs6Z"
   },
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 7400,
     "status": "ok",
     "timestamp": 1748593028689,
     "user": {
      "displayName": "袁再权",
      "userId": "15431997863215513287"
     },
     "user_tz": -480
    },
    "id": "LqcIE-0BNwP4"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"ResNet 的基本残差块\"\"\"\n",
    "    def __init__(self, num_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"支持双输入分支的神经网络\"\"\"\n",
    "    def __init__(self, conv_input_shape=(9,2,4), fc_input_size=11,\n",
    "                 action_size=40, num_res_blocks=5, num_hidden_channels=64):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # 卷积分支处理棋盘状态 (9x2x4)\n",
    "        self.conv_in = nn.Conv2d(conv_input_shape[0], num_hidden_channels,\n",
    "                                kernel_size=3, padding=1, bias=False)\n",
    "        self.bn_in = nn.BatchNorm2d(num_hidden_channels)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(num_hidden_channels)\n",
    "                                        for _ in range(num_res_blocks)])\n",
    "\n",
    "        # 全连接分支处理全局特征 (11维)\n",
    "        self.fc_branch = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 策略头和价值头的公共参数\n",
    "        self.conv_flat_size = num_hidden_channels * conv_input_shape[1] * conv_input_shape[2]\n",
    "        self.combined_features = self.conv_flat_size + 64  # 卷积展平后 + 全连接分支输出\n",
    "\n",
    "        # 策略头\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(self.combined_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_size)\n",
    "        )\n",
    "\n",
    "        # 价值头\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(self.combined_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_conv, x_fc):\n",
    "        # 处理卷积分支\n",
    "        conv_out = F.relu(self.bn_in(self.conv_in(x_conv)))\n",
    "        conv_out = self.res_blocks(conv_out)\n",
    "        conv_flat = conv_out.view(conv_out.size(0), -1)\n",
    "\n",
    "        # 处理全连接分支\n",
    "        fc_out = self.fc_branch(x_fc)\n",
    "\n",
    "        # 特征融合\n",
    "        combined = torch.cat([conv_flat, fc_out], dim=1)\n",
    "\n",
    "        # 生成输出\n",
    "        policy = self.policy_head(combined)\n",
    "        value = self.value_head(combined)\n",
    "        return policy, value\n",
    "\n",
    "    def predict(self, state_np):\n",
    "        \"\"\"处理原始83维状态输入\"\"\"\n",
    "        # 拆分卷积输入 (前72个元素) 和全连接输入 (后11个元素)\n",
    "        x_conv = state_np[:72].reshape(9, 2, 4)\n",
    "        x_fc = state_np[72:83]\n",
    "\n",
    "        # 转换为张量并添加批次维度\n",
    "        x_conv_t = torch.FloatTensor(x_conv).unsqueeze(0).to(next(self.parameters()).device)\n",
    "        x_fc_t = torch.FloatTensor(x_fc).unsqueeze(0).to(next(self.parameters()).device)\n",
    "\n",
    "        # 前向计算\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            policy, value = self.forward(x_conv_t, x_fc_t)\n",
    "        policy_probs = F.softmax(policy, dim=1).cpu().numpy()[0]\n",
    "        value_scalar = value.cpu().numpy()[0][0]\n",
    "        return policy_probs, value_scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjCxuS-kQwAJ"
   },
   "source": [
    "train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12896,
     "status": "ok",
     "timestamp": 1748594366708,
     "user": {
      "displayName": "袁再权",
      "userId": "15431997863215513287"
     },
     "user_tz": -480
    },
    "id": "IL1YCUP9QzzT",
    "outputId": "9650ee3b-2a4c-44bb-ec17-21219279f6e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Iteration 1/100 ===\n",
      "--- Starting Self-Play (Iteration 0) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4844 samples) ---\n",
      " Duration: 2.12s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.2074\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 2/100 ===\n",
      "--- Starting Self-Play (Iteration 1) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4496 samples) ---\n",
      " Duration: 2.06s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.6653\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 3/100 ===\n",
      "--- Starting Self-Play (Iteration 2) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (3980 samples) ---\n",
      " Duration: 1.84s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.3717\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 2\n",
      "\n",
      "=== Iteration 4/100 ===\n",
      "--- Starting Self-Play (Iteration 3) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4384 samples) ---\n",
      " Duration: 1.91s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.4512\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 5/100 ===\n",
      "--- Starting Self-Play (Iteration 4) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4592 samples) ---\n",
      " Duration: 1.98s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.1856\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 6/100 ===\n",
      "--- Starting Self-Play (Iteration 5) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4564 samples) ---\n",
      " Duration: 1.97s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.3850\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 5\n",
      "\n",
      "=== Iteration 7/100 ===\n",
      "--- Starting Self-Play (Iteration 6) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4764 samples) ---\n",
      " Duration: 2.35s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.0631\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 8/100 ===\n",
      "--- Starting Self-Play (Iteration 7) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4992 samples) ---\n",
      " Duration: 2.19s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.1562\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 9/100 ===\n",
      "--- Starting Self-Play (Iteration 8) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4460 samples) ---\n",
      " Duration: 1.94s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.2661\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 8\n",
      "\n",
      "=== Iteration 10/100 ===\n",
      "--- Starting Self-Play (Iteration 9) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4572 samples) ---\n",
      " Duration: 2.10s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.9858\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 11/100 ===\n",
      "--- Starting Self-Play (Iteration 10) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4432 samples) ---\n",
      " Duration: 1.82s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.8988\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 12/100 ===\n",
      "--- Starting Self-Play (Iteration 11) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4804 samples) ---\n",
      " Duration: 1.97s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.9204\n",
      " Duration: 0.02s\n",
      "Checkpoint saved at iteration 11\n",
      "\n",
      "=== Iteration 13/100 ===\n",
      "--- Starting Self-Play (Iteration 12) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4668 samples) ---\n",
      " Duration: 2.01s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 3.0087\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 14/100 ===\n",
      "--- Starting Self-Play (Iteration 13) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4640 samples) ---\n",
      " Duration: 1.95s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.8589\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 15/100 ===\n",
      "--- Starting Self-Play (Iteration 14) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4640 samples) ---\n",
      " Duration: 1.93s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.6387\n",
      " Duration: 0.02s\n",
      "Checkpoint saved at iteration 14\n",
      "\n",
      "=== Iteration 16/100 ===\n",
      "--- Starting Self-Play (Iteration 15) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4920 samples) ---\n",
      " Duration: 2.10s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.7594\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 17/100 ===\n",
      "--- Starting Self-Play (Iteration 16) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4992 samples) ---\n",
      " Duration: 2.09s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.8107\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 18/100 ===\n",
      "--- Starting Self-Play (Iteration 17) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4808 samples) ---\n",
      " Duration: 2.04s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.5731\n",
      " Duration: 0.02s\n",
      "Checkpoint saved at iteration 17\n",
      "\n",
      "=== Iteration 19/100 ===\n",
      "--- Starting Self-Play (Iteration 18) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4456 samples) ---\n",
      " Duration: 2.07s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.4322\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 20/100 ===\n",
      "--- Starting Self-Play (Iteration 19) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4912 samples) ---\n",
      " Duration: 2.07s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.5996\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 21/100 ===\n",
      "--- Starting Self-Play (Iteration 20) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4868 samples) ---\n",
      " Duration: 2.06s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.5761\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 20\n",
      "\n",
      "=== Iteration 22/100 ===\n",
      "--- Starting Self-Play (Iteration 21) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4748 samples) ---\n",
      " Duration: 2.10s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.3974\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 23/100 ===\n",
      "--- Starting Self-Play (Iteration 22) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5384 samples) ---\n",
      " Duration: 2.33s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.2069\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 24/100 ===\n",
      "--- Starting Self-Play (Iteration 23) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4712 samples) ---\n",
      " Duration: 2.04s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.1858\n",
      " Duration: 0.02s\n",
      "Checkpoint saved at iteration 23\n",
      "\n",
      "=== Iteration 25/100 ===\n",
      "--- Starting Self-Play (Iteration 24) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4456 samples) ---\n",
      " Duration: 2.15s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.2079\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 26/100 ===\n",
      "--- Starting Self-Play (Iteration 25) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4908 samples) ---\n",
      " Duration: 2.35s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.4049\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 27/100 ===\n",
      "--- Starting Self-Play (Iteration 26) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5372 samples) ---\n",
      " Duration: 2.29s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.1558\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 26\n",
      "\n",
      "=== Iteration 28/100 ===\n",
      "--- Starting Self-Play (Iteration 27) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5028 samples) ---\n",
      " Duration: 2.18s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.0861\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 29/100 ===\n",
      "--- Starting Self-Play (Iteration 28) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4984 samples) ---\n",
      " Duration: 2.23s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.0215\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 30/100 ===\n",
      "--- Starting Self-Play (Iteration 29) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4724 samples) ---\n",
      " Duration: 2.08s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.4861\n",
      " Duration: 0.02s\n",
      "Checkpoint saved at iteration 29\n",
      "\n",
      "=== Iteration 31/100 ===\n",
      "--- Starting Self-Play (Iteration 30) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4716 samples) ---\n",
      " Duration: 2.14s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.1823\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 32/100 ===\n",
      "--- Starting Self-Play (Iteration 31) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4468 samples) ---\n",
      " Duration: 2.10s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.7462\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 33/100 ===\n",
      "--- Starting Self-Play (Iteration 32) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4892 samples) ---\n",
      " Duration: 2.21s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9173\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 32\n",
      "\n",
      "=== Iteration 34/100 ===\n",
      "--- Starting Self-Play (Iteration 33) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4944 samples) ---\n",
      " Duration: 2.16s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.0952\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 35/100 ===\n",
      "--- Starting Self-Play (Iteration 34) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4600 samples) ---\n",
      " Duration: 2.08s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.7517\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 36/100 ===\n",
      "--- Starting Self-Play (Iteration 35) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4736 samples) ---\n",
      " Duration: 2.11s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.0659\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 35\n",
      "\n",
      "=== Iteration 37/100 ===\n",
      "--- Starting Self-Play (Iteration 36) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4632 samples) ---\n",
      " Duration: 2.01s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.8616\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 38/100 ===\n",
      "--- Starting Self-Play (Iteration 37) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4548 samples) ---\n",
      " Duration: 2.16s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9642\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 39/100 ===\n",
      "--- Starting Self-Play (Iteration 38) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4612 samples) ---\n",
      " Duration: 2.43s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.8271\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 38\n",
      "\n",
      "=== Iteration 40/100 ===\n",
      "--- Starting Self-Play (Iteration 39) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4396 samples) ---\n",
      " Duration: 2.06s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.1218\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 41/100 ===\n",
      "--- Starting Self-Play (Iteration 40) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4536 samples) ---\n",
      " Duration: 2.07s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9523\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 42/100 ===\n",
      "--- Starting Self-Play (Iteration 41) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4976 samples) ---\n",
      " Duration: 2.26s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9164\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 41\n",
      "\n",
      "=== Iteration 43/100 ===\n",
      "--- Starting Self-Play (Iteration 42) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5496 samples) ---\n",
      " Duration: 2.55s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.6542\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 44/100 ===\n",
      "--- Starting Self-Play (Iteration 43) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4864 samples) ---\n",
      " Duration: 2.22s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.7522\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 45/100 ===\n",
      "--- Starting Self-Play (Iteration 44) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5092 samples) ---\n",
      " Duration: 2.32s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.7663\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 44\n",
      "\n",
      "=== Iteration 46/100 ===\n",
      "--- Starting Self-Play (Iteration 45) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4624 samples) ---\n",
      " Duration: 2.14s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9284\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 47/100 ===\n",
      "--- Starting Self-Play (Iteration 46) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4632 samples) ---\n",
      " Duration: 2.02s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.7422\n",
      " Duration: 0.02s\n",
      "\n",
      "=== Iteration 48/100 ===\n",
      "--- Starting Self-Play (Iteration 47) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5360 samples) ---\n",
      " Duration: 2.51s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9509\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 47\n",
      "\n",
      "=== Iteration 49/100 ===\n",
      "--- Starting Self-Play (Iteration 48) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4908 samples) ---\n",
      " Duration: 2.60s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9318\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 50/100 ===\n",
      "--- Starting Self-Play (Iteration 49) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4884 samples) ---\n",
      " Duration: 2.48s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.7106\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 51/100 ===\n",
      "--- Starting Self-Play (Iteration 50) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4796 samples) ---\n",
      " Duration: 2.41s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9676\n",
      " Duration: 0.02s\n",
      "Checkpoint saved at iteration 50\n",
      "\n",
      "=== Iteration 52/100 ===\n",
      "--- Starting Self-Play (Iteration 51) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4988 samples) ---\n",
      " Duration: 2.57s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.0658\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 53/100 ===\n",
      "--- Starting Self-Play (Iteration 52) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5124 samples) ---\n",
      " Duration: 2.54s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.9927\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 54/100 ===\n",
      "--- Starting Self-Play (Iteration 53) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5352 samples) ---\n",
      " Duration: 2.57s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 2.0004\n",
      " Duration: 0.03s\n",
      "Checkpoint saved at iteration 53\n",
      "\n",
      "=== Iteration 55/100 ===\n",
      "--- Starting Self-Play (Iteration 54) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (5388 samples) ---\n",
      " Duration: 2.44s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.8186\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 56/100 ===\n",
      "--- Starting Self-Play (Iteration 55) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n",
      " Completed 50/50 games\n",
      "--- Self-Play Completed (4980 samples) ---\n",
      " Duration: 2.20s\n",
      "--- Starting Training ---\n",
      "--- Training Completed ---\n",
      " Avg Loss: 1.5930\n",
      " Duration: 0.03s\n",
      "\n",
      "=== Iteration 57/100 ===\n",
      "--- Starting Self-Play (Iteration 56) ---\n",
      " Completed 10/50 games\n",
      " Completed 20/50 games\n",
      " Completed 30/50 games\n",
      " Completed 40/50 games\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 320\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iter_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iter, CONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_iterations\u001b[39m\u001b[33m'\u001b[39m]): \u001b[38;5;66;03m# Renamed iter to iter_num to avoid conflict\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_num+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_iterations\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     \u001b[43mrun_self_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     train_network(network, optimizer, replay_buffer)\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (iter_num+\u001b[32m1\u001b[39m) % CONFIG[\u001b[33m'\u001b[39m\u001b[33mcheckpoint_interval\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mrun_self_play\u001b[39m\u001b[34m(network, replay_buffer, iteration)\u001b[39m\n\u001b[32m    165\u001b[39m state_conv_tensor, state_fc_tensor = get_nn_input_from_env(env)\n\u001b[32m    168\u001b[39m current_state_np = env.get_state() \u001b[38;5;66;03m# 直接获取完整的numpy状态给predict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m policy_probs_from_net, _ = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state_np\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 直接从网络获取策略\u001b[39;00m\n\u001b[32m    172\u001b[39m valid_actions = env.valid_actions()\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.sum(valid_actions) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mNeuralNetwork.predict\u001b[39m\u001b[34m(self, state_np)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.eval()\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     policy, value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_conv_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_fc_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m policy_probs = F.softmax(policy, dim=\u001b[32m1\u001b[39m).cpu().numpy()[\u001b[32m0\u001b[39m]\n\u001b[32m     96\u001b[39m value_scalar = value.cpu().numpy()[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mNeuralNetwork.forward\u001b[39m\u001b[34m(self, x_conv, x_fc)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_conv, x_fc):\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# 处理卷积分支\u001b[39;00m\n\u001b[32m     66\u001b[39m     conv_out = F.relu(\u001b[38;5;28mself\u001b[39m.bn_in(\u001b[38;5;28mself\u001b[39m.conv_in(x_conv)))\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     conv_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mres_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     conv_flat = conv_out.view(conv_out.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# 处理全连接分支\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mResidualBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     17\u001b[39m     residual = x\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     out = F.relu(\u001b[38;5;28mself\u001b[39m.bn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     19\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.bn2(\u001b[38;5;28mself\u001b[39m.conv2(out))\n\u001b[32m     20\u001b[39m     out += residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "\n",
    "# --- Configuration --- (保持不变)\n",
    "CONFIG = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "    'num_iterations': 100,\n",
    "    'num_self_play_games': 50,\n",
    "    'replay_buffer_size': 50000,\n",
    "    'train_batch_size': 128,\n",
    "    'learning_rate': 0.001,\n",
    "    'temperature_initial': 1.0,\n",
    "    'temperature_final': 0.1,\n",
    "    'temperature_decay_steps': 30,\n",
    "    'checkpoint_interval': 3,\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    'replay_buffer_path': './replay_buffer.pkl',\n",
    "    'max_game_moves': 100\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_nn_input_from_env(env: GameEnvironment):\n",
    "    \"\"\"Convert environment state to network input tensors\"\"\"\n",
    "    state_np = env.get_state()\n",
    "    x_conv = state_np[:72].reshape(9, 2, 4).astype(np.float32)\n",
    "    x_fc = state_np[72:83].astype(np.float32)\n",
    "    return (\n",
    "        torch.tensor(x_conv, dtype=torch.float32),\n",
    "        torch.tensor(x_fc, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "def get_temperature(iteration):\n",
    "    if iteration < CONFIG['temperature_decay_steps']:\n",
    "        return CONFIG['temperature_initial'] - (CONFIG['temperature_initial'] - CONFIG['temperature_final']) * (iteration / CONFIG['temperature_decay_steps'])\n",
    "    else:\n",
    "        return CONFIG['temperature_final']\n",
    "\n",
    "# --- Replay Buffer & Dataset ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(list(self.buffer), min(batch_size, len(self.buffer)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.buffer, f)\n",
    "\n",
    "    def load(self, path):\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                self.buffer = pickle.load(f)\n",
    "\n",
    "class ActionHistoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.states_conv = [item[0] for item in data]\n",
    "        self.states_fc = [item[1] for item in data]\n",
    "        self.pis = [torch.tensor(item[2], dtype=torch.float32) for item in data]\n",
    "        self.zs = [torch.tensor([item[3]], dtype=torch.float32) for item in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states_conv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.states_conv[idx],\n",
    "            self.states_fc[idx],\n",
    "            self.pis[idx],\n",
    "            self.zs[idx]\n",
    "        )\n",
    "# 训练经验扩充 (apply_symmetry 保持不变)\n",
    "def apply_symmetry(state_conv_tensor, original_pi, symmetry_type):\n",
    "    \"\"\"应用对称变换到棋盘状态和动作概率\"\"\"\n",
    "    if symmetry_type == 'none':\n",
    "        return state_conv_tensor.clone(), original_pi.copy()\n",
    "\n",
    "    transformed_pi = np.zeros_like(original_pi)\n",
    "\n",
    "    if symmetry_type == 'flip_row':\n",
    "        transformed_conv = torch.flip(state_conv_tensor.clone(), dims=[1])\n",
    "        row_trans = lambda r: 1 - r\n",
    "        col_trans = lambda c: c\n",
    "        action_map = {\n",
    "            0: 1,\n",
    "            1: 0,\n",
    "            2: 2,\n",
    "            3: 3,\n",
    "            4: 4\n",
    "        }\n",
    "    elif symmetry_type == 'flip_col':\n",
    "        transformed_conv = torch.flip(state_conv_tensor.clone(), dims=[2])\n",
    "        row_trans = lambda r: r\n",
    "        col_trans = lambda c: 3 - c\n",
    "        action_map = {\n",
    "            0: 0,\n",
    "            1: 1,\n",
    "            2: 3,\n",
    "            3: 2,\n",
    "            4: 4\n",
    "        }\n",
    "    elif symmetry_type == 'flip_both':\n",
    "        transformed_conv = torch.flip(state_conv_tensor.clone(), dims=[1, 2])\n",
    "        row_trans = lambda r: 1 - r\n",
    "        col_trans = lambda c: 3 - c\n",
    "        action_map = {\n",
    "            0: 1,\n",
    "            1: 0,\n",
    "            2: 3,\n",
    "            3: 2,\n",
    "            4: 4\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"不支持的对称类型\")\n",
    "\n",
    "    for action_idx in np.where(original_pi > 0)[0]:\n",
    "        pos_idx = action_idx // 5\n",
    "        sub_action = action_idx % 5\n",
    "        original_row = pos_idx // 4\n",
    "        original_col = pos_idx % 4\n",
    "        new_row = row_trans(original_row)\n",
    "        new_col = col_trans(original_col)\n",
    "        new_pos_idx = new_row * 4 + new_col\n",
    "        new_sub_action = action_map[sub_action]\n",
    "        new_action_idx = new_pos_idx * 5 + new_sub_action\n",
    "        if 0 <= new_action_idx < 40:\n",
    "            transformed_pi[new_action_idx] = original_pi[action_idx]\n",
    "\n",
    "    return transformed_conv, transformed_pi\n",
    "\n",
    "# --- Self-Play ---\n",
    "def run_self_play(network, replay_buffer, iteration):\n",
    "    print(f\"--- Starting Self-Play (Iteration {iteration}) ---\")\n",
    "    network.eval()\n",
    "    new_experiences = []\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    for game_num in range(CONFIG['num_self_play_games']):\n",
    "        env = GameEnvironment()\n",
    "        game_history = []\n",
    "        move_count = 0\n",
    "        done = False\n",
    "\n",
    "        while not done and move_count < CONFIG['max_game_moves']:\n",
    "\n",
    "            state_conv_tensor, state_fc_tensor = get_nn_input_from_env(env)\n",
    "\n",
    "\n",
    "            current_state_np = env.get_state() # 直接获取完整的numpy状态给predict\n",
    "\n",
    "            policy_probs_from_net, _ = network.predict(current_state_np) # 直接从网络获取策略\n",
    "\n",
    "            valid_actions = env.valid_actions()\n",
    "\n",
    "            if np.sum(valid_actions) == 0:\n",
    "                print(f\"Game {game_num+1}: No valid actions available, ending game.\")\n",
    "                break # 没有有效动作，游戏结束\n",
    "\n",
    "            # 屏蔽无效动作\n",
    "            masked_policy_probs = policy_probs_from_net * valid_actions\n",
    "\n",
    "            # 归一化处理，并准备用于历史记录的pi\n",
    "            if np.sum(masked_policy_probs) > 1e-8:\n",
    "                pi_for_history = masked_policy_probs / np.sum(masked_policy_probs)\n",
    "            else:\n",
    "                # 如果所有有效动作概率为0，则在有效动作中均匀选择\n",
    "                pi_for_history = valid_actions / np.sum(valid_actions)\n",
    "\n",
    "            # 根据温度选择动作\n",
    "            temp = get_temperature(iteration)\n",
    "            if temp == 0: # 确定性选择，用于后期或评估\n",
    "                action_idx = np.argmax(pi_for_history)\n",
    "            else:\n",
    "                # 带温度的随机抽样\n",
    "                # 注意：pi_for_history 必须是归一化的概率分布\n",
    "                # 如果 pi_for_history 可能不是严格的概率分布（例如，元素和不为1），需要再次归一化\n",
    "                if not np.isclose(np.sum(pi_for_history), 1.0):\n",
    "                     if np.sum(pi_for_history) > 1e-8 :\n",
    "                         pi_for_choice = pi_for_history / np.sum(pi_for_history)\n",
    "                     else: # 再次检查，如果还是和为0，则均匀分布\n",
    "                         pi_for_choice = valid_actions / np.sum(valid_actions)\n",
    "                else:\n",
    "                    pi_for_choice = pi_for_history\n",
    "\n",
    "                try:\n",
    "                    action_idx = np.random.choice(len(pi_for_choice), p=pi_for_choice)\n",
    "                except ValueError: # 如果p的和不为1会出错\n",
    "                    # Fallback: uniformly random among valid actions\n",
    "                    valid_indices = np.where(valid_actions == 1)[0]\n",
    "                    action_idx = np.random.choice(valid_indices)\n",
    "\n",
    "\n",
    "            game_history.append((state_conv_tensor, state_fc_tensor, pi_for_history, env.current_player))\n",
    "\n",
    "            _, current_player, winner, done = env.step(action_idx)\n",
    "            move_count += 1\n",
    "\n",
    "\n",
    "        # Determine final outcome (保持不变)\n",
    "        if done:\n",
    "            game_outcome = winner\n",
    "        else:\n",
    "            if env.scores[1] > env.scores[-1]:\n",
    "                game_outcome = 1\n",
    "            elif env.scores[-1] > env.scores[1]:\n",
    "                game_outcome = -1\n",
    "            else:\n",
    "                game_outcome = 0\n",
    "\n",
    "        decay_factor = CONFIG.get('reward_decay', 0.98)\n",
    "        game_history_len=len(game_history)\n",
    "        num_steps_to_end = game_history_len - 1\n",
    "        # Record experiences (保持不变, 注意 state_conv 和 state_fc 的来源)\n",
    "        for state_conv, state_fc, pi, player_hist_turn in game_history:\n",
    "            z = game_outcome * player_hist_turn * (decay_factor ** num_steps_to_end)\n",
    "            num_steps_to_end = num_steps_to_end - 1\n",
    "            new_experiences.append((\n",
    "                state_conv.cpu(),\n",
    "                state_fc.cpu(),\n",
    "                pi,\n",
    "                z\n",
    "            ))\n",
    "\n",
    "            for symmetry in ['flip_row', 'flip_col', 'flip_both']:\n",
    "                sym_conv, sym_pi = apply_symmetry(state_conv, pi, symmetry)\n",
    "                new_experiences.append((\n",
    "                    sym_conv.cpu(),\n",
    "                    state_fc.cpu(),\n",
    "                    sym_pi,\n",
    "                    z\n",
    "                ))\n",
    "        if (game_num+1) % 10 == 0:\n",
    "            print(f\" Completed {game_num+1}/{CONFIG['num_self_play_games']} games\")\n",
    "\n",
    "    replay_buffer.add(new_experiences)\n",
    "    print(f\"--- Self-Play Completed ({len(new_experiences)} samples) ---\")\n",
    "    print(f\" Duration: {time.time()-start_time:.2f}s\")\n",
    "\n",
    "# --- Training --- (train_network 保持不变)\n",
    "def train_network(network, optimizer, replay_buffer):\n",
    "    if len(replay_buffer) < CONFIG['train_batch_size']:\n",
    "        print(\"Not enough samples for training\")\n",
    "        return\n",
    "\n",
    "    print(\"--- Starting Training ---\")\n",
    "    network.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    sampled_data = replay_buffer.sample(CONFIG['train_batch_size'])\n",
    "    dataset = ActionHistoryDataset(sampled_data)\n",
    "    loader = DataLoader(dataset, batch_size=CONFIG['train_batch_size'], shuffle=True)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        states_conv, states_fc, target_pis, target_zs = batch\n",
    "        states_conv = states_conv.to(CONFIG['device'])\n",
    "        states_fc = states_fc.to(CONFIG['device'])\n",
    "        target_pis = target_pis.to(CONFIG['device'])\n",
    "        target_zs = target_zs.to(CONFIG['device'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_logits, value_preds = network(states_conv, states_fc)\n",
    "\n",
    "        log_probs = F.log_softmax(policy_logits, dim=1)\n",
    "        policy_loss = F.kl_div(log_probs, target_pis, reduction='batchmean')\n",
    "        value_loss = F.mse_loss(value_preds.squeeze(), target_zs.squeeze())\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"--- Training Completed ---\")\n",
    "    print(f\" Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\" Duration: {time.time()-start_time:.2f}s\")\n",
    "\n",
    "\n",
    "# --- Main Loop --- (保持不变)\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using device: {CONFIG['device']}\")\n",
    "    os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "    network = NeuralNetwork().to(CONFIG['device'])\n",
    "    optimizer = optim.Adam(network.parameters(), lr=CONFIG['learning_rate'])\n",
    "    replay_buffer = ReplayBuffer(CONFIG['replay_buffer_size'])\n",
    "\n",
    "    start_iter = 0\n",
    "    checkpoint_path = os.path.join(CONFIG['checkpoint_dir'], \"latest.pth\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        network.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_iter = checkpoint['iteration'] + 1\n",
    "        replay_buffer.load(CONFIG['replay_buffer_path'])\n",
    "        print(f\"Resuming from iteration {start_iter}\")\n",
    "\n",
    "    for iter_num in range(start_iter, CONFIG['num_iterations']): # Renamed iter to iter_num to avoid conflict\n",
    "        print(f\"\\n=== Iteration {iter_num+1}/{CONFIG['num_iterations']} ===\")\n",
    "\n",
    "        run_self_play(network, replay_buffer, iter_num)\n",
    "\n",
    "        train_network(network, optimizer, replay_buffer)\n",
    "\n",
    "        if (iter_num+1) % CONFIG['checkpoint_interval'] == 0:\n",
    "            torch.save({\n",
    "                'iteration': iter_num,\n",
    "                'model': network.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }, checkpoint_path)\n",
    "            replay_buffer.save(CONFIG['replay_buffer_path'])\n",
    "\n",
    "            print(f\"Checkpoint saved at iteration {iter_num}\")\n",
    "\n",
    "    print(\"\\n=== Training Completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32036,
     "status": "ok",
     "timestamp": 1748594612496,
     "user": {
      "displayName": "袁再权",
      "userId": "15431997863215513287"
     },
     "user_tz": -480
    },
    "id": "LxupqBDMQt1L",
    "outputId": "38bbd358-2ef6-4797-e274-2f2d5f6fea83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始评估...\n",
      "已完成 2/20 局对战\n",
      "已完成 4/20 局对战\n",
      "已完成 6/20 局对战\n",
      "已完成 8/20 局对战\n",
      "已完成 10/20 局对战\n",
      "已完成 12/20 局对战\n",
      "已完成 14/20 局对战\n",
      "已完成 16/20 局对战\n",
      "已完成 18/20 局对战\n",
      "已完成 20/20 局对战\n",
      "\n",
      "评估结果:\n",
      "AI 胜局: 6 (30.0%)\n",
      "随机策略胜局: 10 (50.0%)\n",
      "平局: 4 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# eval.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"加载训练好的模型\"\"\"\n",
    "    model = NeuralNetwork().to(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class AIPlayer:\n",
    "    \"\"\"基于神经网络直接输出的AI玩家\"\"\"\n",
    "    def __init__(self, model): # config 参数可能不再需要，除非有其他配置项\n",
    "        self.model = model\n",
    "\n",
    "    def get_action(self, env):\n",
    "        # 获取当前状态\n",
    "        state_np = env.get_state() # [cite: 2]\n",
    "\n",
    "        # 使用模型预测策略概率\n",
    "        # model.predict 返回 policy_probs (已经过softmax) 和 value\n",
    "        policy_probs, _ = self.model.predict(state_np) # [cite: 6]\n",
    "\n",
    "        # 获取有效动作\n",
    "        valid_actions = env.valid_actions() # [cite: 2]\n",
    "\n",
    "        # 屏蔽无效动作的概率\n",
    "        masked_policy_probs = policy_probs * valid_actions # [cite: 5]\n",
    "\n",
    "        # 检查是否有任何有效动作的概率大于0\n",
    "        if np.sum(masked_policy_probs) > 1e-8:\n",
    "            # 在评估时，选择概率最高的动作 (确定性策略)\n",
    "            action = np.argmax(masked_policy_probs)\n",
    "        else:\n",
    "            # 如果所有有效动作的预测概率都非常低（或为0），\n",
    "            # 则从有效动作中随机选择一个，或选择第一个作为后备。\n",
    "            # 这有助于避免在网络未充分训练时卡住。\n",
    "            valid_indices = np.where(valid_actions == 1)[0]\n",
    "            if len(valid_indices) > 0:\n",
    "                action = np.random.choice(valid_indices)\n",
    "            else:\n",
    "                # 如果没有有效动作（理论上游戏应已结束或即将结束）\n",
    "                # 可以返回一个特殊值或尝试找到任何一个概率不为0的动作\n",
    "                # 但在实际中，若无有效动作，游戏逻辑会处理。\n",
    "                # 为安全起见，如果真的发生，返回第一个动作索引（尽管它可能是无效的）\n",
    "                # 或引发错误，或让游戏环境决定。\n",
    "                # 更好的做法是依赖游戏环境在无有效动作时结束游戏。\n",
    "                # 这里假设游戏总有有效动作，或者游戏结束逻辑会先触发。\n",
    "                # 如果执行到这里说明 valid_actions 全是0，则 argmax(masked_policy_probs) 也会是0\n",
    "                action = np.argmax(masked_policy_probs) # 默认为0\n",
    "\n",
    "        return action\n",
    "\n",
    "class RandomPlayer:\n",
    "    \"\"\"随机策略玩家\"\"\"\n",
    "    def get_action(self, env):\n",
    "        valid_actions = env.valid_actions()\n",
    "        valid_indices = np.where(valid_actions == 1)[0]\n",
    "        return np.random.choice(valid_indices)\n",
    "\n",
    "def play_game(ai_player, random_player, ai_plays_as=1):\n",
    "    \"\"\"进行一局游戏\"\"\"\n",
    "    env = GameEnvironment()\n",
    "    players = {1: ai_player if ai_plays_as == 1 else random_player,\n",
    "               -1: random_player if ai_plays_as == 1 else ai_player}\n",
    "\n",
    "    while True:\n",
    "        current_player_obj = players[env.current_player]\n",
    "        action = current_player_obj.get_action(env)\n",
    "\n",
    "        _, _, winner, done = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            # 转换胜利结果到玩家视角\n",
    "            if winner == ai_plays_as:\n",
    "                return 1  # AI胜\n",
    "            elif winner == -ai_plays_as:\n",
    "                return -1  # 随机玩家胜\n",
    "            else:\n",
    "                return 0  # 平局\n",
    "\n",
    "def evaluate(ai_model, num_games=100):\n",
    "    \"\"\"评估函数\"\"\"\n",
    "    results = defaultdict(int)\n",
    "    ai_player = AIPlayer(ai_model)\n",
    "    random_player = RandomPlayer()\n",
    "\n",
    "    print(\"开始评估...\")\n",
    "    for i in range(num_games):\n",
    "        # 交替先手\n",
    "        if i % 2 == 0:\n",
    "            result = play_game(ai_player, random_player, ai_plays_as=1)\n",
    "        else:\n",
    "            result = play_game(ai_player, random_player, ai_plays_as=-1)\n",
    "\n",
    "        if result == 1:\n",
    "            results['ai_wins'] += 1\n",
    "        elif result == -1:\n",
    "            results['random_wins'] += 1\n",
    "        else:\n",
    "            results['draws'] += 1\n",
    "\n",
    "        if (i+1) % (num_games//10) == 0:\n",
    "            print(f\"已完成 {i+1}/{num_games} 局对战\")\n",
    "\n",
    "    # 计算胜率\n",
    "    total = num_games\n",
    "    print(\"\\n评估结果:\")\n",
    "    print(f\"AI 胜局: {results['ai_wins']} ({results['ai_wins']/total:.1%})\")\n",
    "    print(f\"随机策略胜局: {results['random_wins']} ({results['random_wins']/total:.1%})\")\n",
    "    print(f\"平局: {results['draws']} ({results['draws']/total:.1%})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = load_model('./checkpoints/latest.pth', device)\n",
    "    evaluate(model, num_games=20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOSWqK0808vfhw04PTTtbKw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
